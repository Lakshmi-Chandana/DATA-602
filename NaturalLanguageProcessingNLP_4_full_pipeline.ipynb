{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "NaturalLanguageProcessingNLP-4-full_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lakshmi-Chandana/DATA-602/blob/main/NaturalLanguageProcessingNLP_4_full_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ5Y0c0kf30x"
      },
      "source": [
        "# Full NLP and ML Pipeline for Document Classification\n",
        "Based on following tutorials   \n",
        "With Permission from Michale Harmon:\n",
        "http://michael-harmon.com/blog/NLP.html   \n",
        "https://github.com/mdh266/DocumentClassificationNLP/blob/master/NLP.ipynb   \n",
        "\n",
        "Classification of text documents using sparse features:   \n",
        "https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html   \n",
        "\n",
        "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSUr-zBCf301"
      },
      "source": [
        "###  20 News Groups Corpus, Sample dataset included in scikit-learn\n",
        "A collection of almost 20,000 articles on 20 different topics or 'newsgroups'.   \n",
        "Corpus: Text Collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xflDPl9mf302"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO-kcPr6f303",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fadfc10-62b6-4ae1-b7c1-5655bbe624c2"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNVr6Wpaf303",
        "outputId": "3be46fa8-6ae3-42a5-a2a3-0ff337c65bfc"
      },
      "source": [
        "# first 3 classes (of 20)\n",
        "twenty_train.target_names[0:3]\n",
        "# python indexing excludes end index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSW0lMZuf304",
        "outputId": "b54e28d7-fe93-4cb0-cffd-15f50ef228fa"
      },
      "source": [
        "# data and target\n",
        "# has the input and desired output\n",
        "# 11K of them are split for training pairs\n",
        "\n",
        "print( len(twenty_train.data) )\n",
        "print( len(twenty_train.target) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11314\n",
            "11314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLcRPWJdf304",
        "outputId": "83752dc0-b32c-439e-a062-34aec39e301a"
      },
      "source": [
        "i = 29\n",
        "print(twenty_train.data[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: jimf@centerline.com (Jim Frost)\n",
            "Subject: Re: Is car saftey important?\n",
            "Organization: CenterLine Software, Inc.\n",
            "Lines: 14\n",
            "NNTP-Posting-Host: 140.239.3.202\n",
            "\n",
            "tcorkum@bnr.ca (Trevor Corkum) writes:\n",
            ">Is it only me, or is\n",
            ">safety not one of the most important factors when buying a car?\n",
            "\n",
            "It depends on your priorities.  A lot of people put higher priorities\n",
            "on gas mileage and cost than on safety, buying \"unsafe\" econoboxes\n",
            "instead of Volvos.  I personally take a middle ground -- the only\n",
            "thing I really look for is a three-point seatbelt and 5+mph bumpers.\n",
            "I figure that 30mph collisions into brick walls aren't common enough\n",
            "for me to spend that much extra money for protection, but there are\n",
            "lots of low-speed collisions that do worry me.\n",
            "\n",
            "jim frost\n",
            "jimf@centerline.com\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otAYlTkhf305",
        "outputId": "245761d9-c1d6-43ea-c5ed-59097392a709"
      },
      "source": [
        "twenty_train.target[i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KAnQ7rpFf306",
        "outputId": "ace20f3a-ca0d-466f-b241-9090efb58ae6"
      },
      "source": [
        "twenty_train.target_names[  twenty_train.target[i]  ]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'rec.autos'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOD57umyf307"
      },
      "source": [
        "## scikit-learn Pipeline\n",
        "- Scitkit-learn pipelines are a sequence of transforms followed by a final estimator.   \n",
        "- Intermediate steps within the pipeline must be ‘transforms’ \n",
        " * they must implement fit and transform methods \n",
        " * The CountVectorizer and TfidfTransformer are transformers in this example   \n",
        "- The estimator of a pipeline, the final step, only needs to implement the fit method   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIlc3Pxeghc-"
      },
      "source": [
        "### A simple pipeline with two steps, count vectorizer and model that uses the count vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2fmi0vwf307",
        "outputId": "b941b9d2-62a0-451f-939a-60e54fd1af7c"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "pipe = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
        "                 ('model', MultinomialNB()),])\n",
        "\n",
        "mod = pipe.fit(twenty_train.data, twenty_train.target)\n",
        "\n",
        "predicted = mod.predict(twenty_test.data)\n",
        "\n",
        "print(classification_report(twenty_test.target,\n",
        "                            predicted, \n",
        "                            target_names=twenty_test.target_names))\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy:\", accuracy_score(twenty_test.target, predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.80      0.81      0.80       319\n",
            "           comp.graphics       0.65      0.80      0.72       389\n",
            " comp.os.ms-windows.misc       0.80      0.04      0.08       394\n",
            "comp.sys.ibm.pc.hardware       0.55      0.80      0.65       392\n",
            "   comp.sys.mac.hardware       0.85      0.79      0.82       385\n",
            "          comp.windows.x       0.69      0.84      0.76       395\n",
            "            misc.forsale       0.89      0.74      0.81       390\n",
            "               rec.autos       0.89      0.92      0.91       396\n",
            "         rec.motorcycles       0.95      0.94      0.95       398\n",
            "      rec.sport.baseball       0.95      0.92      0.93       397\n",
            "        rec.sport.hockey       0.92      0.97      0.94       399\n",
            "               sci.crypt       0.80      0.96      0.87       396\n",
            "         sci.electronics       0.79      0.70      0.74       393\n",
            "                 sci.med       0.88      0.87      0.87       396\n",
            "               sci.space       0.84      0.92      0.88       394\n",
            "  soc.religion.christian       0.81      0.95      0.87       398\n",
            "      talk.politics.guns       0.72      0.93      0.81       364\n",
            "   talk.politics.mideast       0.93      0.94      0.94       376\n",
            "      talk.politics.misc       0.68      0.62      0.65       310\n",
            "      talk.religion.misc       0.88      0.44      0.59       251\n",
            "\n",
            "                accuracy                           0.80      7532\n",
            "               macro avg       0.81      0.79      0.78      7532\n",
            "            weighted avg       0.81      0.80      0.78      7532\n",
            "\n",
            "Accuracy: 0.8023101433882103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4VdyOwLbhmo"
      },
      "source": [
        "### Adding tf-idf transformation to the count vectors\n",
        "\n",
        "I used these 2 steps in the pipe to give an example. These first 2 stesp are equivalent to TfidfVectorizer. https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html says: \"Equivalent to CountVectorizer followed by TfidfTransformer.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKY-iV4sf308",
        "outputId": "f970ef0f-647f-44e0-c18c-45a425bc5624"
      },
      "source": [
        "# Easyly to add, remove, or modify steps and retest \n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "pipe = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
        "                  ('tfidf', TfidfTransformer()), # Added TFIDF\n",
        "                  ('model', MultinomialNB()),])\n",
        "\n",
        "mod = pipe.fit(twenty_train.data, twenty_train.target)\n",
        "\n",
        "predicted = mod.predict(twenty_test.data)\n",
        "\n",
        "print(classification_report(twenty_test.target,\n",
        "                            predicted, \n",
        "                            target_names=twenty_test.target_names))\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy:\", accuracy_score(twenty_test.target, predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.80      0.69      0.74       319\n",
            "           comp.graphics       0.78      0.72      0.75       389\n",
            " comp.os.ms-windows.misc       0.79      0.72      0.75       394\n",
            "comp.sys.ibm.pc.hardware       0.68      0.81      0.74       392\n",
            "   comp.sys.mac.hardware       0.86      0.81      0.84       385\n",
            "          comp.windows.x       0.87      0.78      0.82       395\n",
            "            misc.forsale       0.87      0.80      0.83       390\n",
            "               rec.autos       0.88      0.91      0.90       396\n",
            "         rec.motorcycles       0.93      0.96      0.95       398\n",
            "      rec.sport.baseball       0.91      0.92      0.92       397\n",
            "        rec.sport.hockey       0.88      0.98      0.93       399\n",
            "               sci.crypt       0.75      0.96      0.84       396\n",
            "         sci.electronics       0.84      0.65      0.74       393\n",
            "                 sci.med       0.92      0.79      0.85       396\n",
            "               sci.space       0.82      0.94      0.88       394\n",
            "  soc.religion.christian       0.62      0.96      0.76       398\n",
            "      talk.politics.guns       0.66      0.95      0.78       364\n",
            "   talk.politics.mideast       0.95      0.94      0.94       376\n",
            "      talk.politics.misc       0.94      0.52      0.67       310\n",
            "      talk.religion.misc       0.95      0.24      0.38       251\n",
            "\n",
            "                accuracy                           0.82      7532\n",
            "               macro avg       0.84      0.80      0.80      7532\n",
            "            weighted avg       0.83      0.82      0.81      7532\n",
            "\n",
            "Accuracy: 0.8169144981412639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_irgn4T5IaZS"
      },
      "source": [
        "# from sklearn.svm import SVC\n",
        "\n",
        "# # Easyly to add, remove, or modify steps and retest \n",
        "# pipe = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
        "#                   ('tfidf', TfidfTransformer()), # Added TFIDF\n",
        "#                   ('model', SVC()),])\n",
        "\n",
        "# mod = pipe.fit(twenty_train.data, twenty_train.target)\n",
        "\n",
        "# predicted = mod.predict(twenty_test.data)\n",
        "\n",
        "# print(classification_report(twenty_test.target,\n",
        "#                             predicted, \n",
        "#                             target_names=twenty_test.target_names))\n",
        "\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# print(\"Accuracy:\", accuracy_score(twenty_test.target, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE3KMY42f309"
      },
      "source": [
        "## Experimenting, and Hyperparameter tuning using GridSearchCV and Pipeline\n",
        "Similar to testing whether to remove stopwords or not, we want to run many experiments, with many combinations of parameters.  \n",
        "GridSearchCV does this, for an estimator, or on a full pipeline. \n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html   \n",
        "   \n",
        "For  setting parameters of the various steps in the \n",
        "pipeline, you use <step name>+\"__\"+<parameter name>, for the list of possible values you want to test.\n",
        "\n",
        "#### Example Experiment; Effect of removing stop_words on accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MznZ-ePf309"
      },
      "source": [
        "# Configure experiments\n",
        "    \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "pipe = Pipeline([('vect', CountVectorizer()),\n",
        "                  ('tfidf', TfidfTransformer()),\n",
        "                  ('model', MultinomialNB()),])\n",
        "\n",
        "### stop_words is a parameter for CountVectorizer\n",
        "### since as a step in pipeline its name is vect, \n",
        "### the key vect__stop_words contains the options we want to experiment\n",
        "### in the parameters dictionary: \n",
        "parameters = {'vect__stop_words': ('english', None)}\n",
        "\n",
        "# We can perform the grid search using all available CPU by setting n_jobs=-1:\n",
        "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwVYhlpYf30-",
        "outputId": "d018a742-ab63-4691-d23e-b005e2555af0"
      },
      "source": [
        "# Run the experiments \n",
        "grid_search.fit(twenty_train.data, twenty_train.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   32.9s finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vect',\n",
              "                                        CountVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.int64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        preprocessor=None,\n",
              "                                                        stop_words='english',\n",
              "                                                        strip_accents=None,\n",
              "                                                        token_pattern=...\n",
              "                                                        tokenizer=None,\n",
              "                                                        vocabulary=None)),\n",
              "                                       ('tfidf',\n",
              "                                        TfidfTransformer(norm='l2',\n",
              "                                                         smooth_idf=True,\n",
              "                                                         sublinear_tf=False,\n",
              "                                                         use_idf=True)),\n",
              "                                       ('model',\n",
              "                                        MultinomialNB(alpha=1.0,\n",
              "                                                      class_prior=None,\n",
              "                                                      fit_prior=True))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'vect__stop_words': ('english', None)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "427xVQ5-f30-",
        "outputId": "861b18a1-a995-43d3-b0e1-6adb48371fd3"
      },
      "source": [
        "# Results of \n",
        "grid_search.cv_results_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([4.96422958, 5.16370244]),\n",
              " 'mean_score_time': array([1.06413231, 1.09180346]),\n",
              " 'mean_test_score': array([0.88165116, 0.84399858]),\n",
              " 'param_vect__stop_words': masked_array(data=['english', None],\n",
              "              mask=[False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'vect__stop_words': 'english'}, {'vect__stop_words': None}],\n",
              " 'rank_test_score': array([1, 2], dtype=int32),\n",
              " 'split0_test_score': array([0.88422448, 0.84887318]),\n",
              " 'split1_test_score': array([0.88378259, 0.84180292]),\n",
              " 'split2_test_score': array([0.88024746, 0.84401237]),\n",
              " 'split3_test_score': array([0.87715422, 0.84136103]),\n",
              " 'split4_test_score': array([0.88284704, 0.84394341]),\n",
              " 'std_fit_time': array([0.31819013, 0.05190805]),\n",
              " 'std_score_time': array([0.08349578, 0.17589451]),\n",
              " 'std_test_score': array([0.00263772, 0.00266618])}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFVg3dmvf30-"
      },
      "source": [
        "As GridSearchCV already does cross validation, we could combine train and test data and feed all to GridSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL5tu4Iaf30_",
        "outputId": "a556944c-2265-4eb2-8e85-ec41b39bba3c"
      },
      "source": [
        "grid_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words='english', strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('model',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpKQGXHPf30_",
        "outputId": "10d87614-2e1b-4cd3-f232-60a05e010217"
      },
      "source": [
        "parameters2 = {'tfidf__use_idf': (True, False),\n",
        "              'model__alpha': (1e1, 1e-3),\n",
        "              'model__fit_prior': (True,False)}\n",
        "\n",
        "\n",
        "grid_search2 = GridSearchCV(pipe, parameters2, n_jobs=-1, cv=5)\n",
        "grid_search2.fit(twenty_train.data, twenty_train.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vect',\n",
              "                                        CountVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.int64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        preprocessor=None,\n",
              "                                                        stop_words='english',\n",
              "                                                        strip_accents=None,\n",
              "                                                        token_pattern=...\n",
              "                                        TfidfTransformer(norm='l2',\n",
              "                                                         smooth_idf=True,\n",
              "                                                         sublinear_tf=False,\n",
              "                                                         use_idf=True)),\n",
              "                                       ('model',\n",
              "                                        MultinomialNB(alpha=1.0,\n",
              "                                                      class_prior=None,\n",
              "                                                      fit_prior=True))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'model__alpha': (10.0, 0.001),\n",
              "                         'model__fit_prior': (True, False),\n",
              "                         'tfidf__use_idf': (True, False)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxjB3x7ff30_",
        "outputId": "482fc850-8db1-4b10-fba5-79504f85f8a7"
      },
      "source": [
        "grid_search2.cv_results_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([4.70247774, 4.76134863, 4.88082309, 4.91927133, 5.05858846,\n",
              "        4.58875818, 4.89991722, 4.62617698]),\n",
              " 'mean_score_time': array([1.00830226, 1.05477238, 1.20125141, 1.1564826 , 1.14066358,\n",
              "        1.03355842, 1.09082742, 0.95299568]),\n",
              " 'mean_test_score': array([0.82066512, 0.75702699, 0.85451692, 0.78133328, 0.90772501,\n",
              "        0.91161416, 0.90754825, 0.91223293]),\n",
              " 'param_model__alpha': masked_array(data=[10.0, 10.0, 10.0, 10.0, 0.001, 0.001, 0.001, 0.001],\n",
              "              mask=[False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_model__fit_prior': masked_array(data=[True, True, False, False, True, True, False, False],\n",
              "              mask=[False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_tfidf__use_idf': masked_array(data=[True, False, True, False, True, False, True, False],\n",
              "              mask=[False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'model__alpha': 10.0,\n",
              "   'model__fit_prior': True,\n",
              "   'tfidf__use_idf': True},\n",
              "  {'model__alpha': 10.0, 'model__fit_prior': True, 'tfidf__use_idf': False},\n",
              "  {'model__alpha': 10.0, 'model__fit_prior': False, 'tfidf__use_idf': True},\n",
              "  {'model__alpha': 10.0, 'model__fit_prior': False, 'tfidf__use_idf': False},\n",
              "  {'model__alpha': 0.001, 'model__fit_prior': True, 'tfidf__use_idf': True},\n",
              "  {'model__alpha': 0.001, 'model__fit_prior': True, 'tfidf__use_idf': False},\n",
              "  {'model__alpha': 0.001, 'model__fit_prior': False, 'tfidf__use_idf': True},\n",
              "  {'model__alpha': 0.001, 'model__fit_prior': False, 'tfidf__use_idf': False}],\n",
              " 'rank_test_score': array([6, 8, 5, 7, 3, 2, 4, 1], dtype=int32),\n",
              " 'split0_test_score': array([0.81573133, 0.75209898, 0.84577994, 0.78082192, 0.91162174,\n",
              "        0.91117985, 0.91029607, 0.91117985]),\n",
              " 'split1_test_score': array([0.817057  , 0.76358816, 0.85240831, 0.78082192, 0.90676094,\n",
              "        0.91073796, 0.90720283, 0.91206363]),\n",
              " 'split2_test_score': array([0.81528944, 0.75033142, 0.85240831, 0.77949624, 0.90013257,\n",
              "        0.90631905, 0.90013257, 0.90720283]),\n",
              " 'split3_test_score': array([0.82942996, 0.75872735, 0.8630137 , 0.77949624, 0.91162174,\n",
              "        0.91559876, 0.91162174, 0.91515687]),\n",
              " 'split4_test_score': array([0.82581786, 0.76038904, 0.85897436, 0.78603006, 0.90848806,\n",
              "        0.91423519, 0.90848806, 0.91556145]),\n",
              " 'std_fit_time': array([0.16698083, 0.10640935, 0.09936239, 0.24437882, 0.19753465,\n",
              "        0.11226535, 0.27699501, 0.11850062]),\n",
              " 'std_score_time': array([0.04344624, 0.05229711, 0.1167746 , 0.10881915, 0.09368064,\n",
              "        0.09254342, 0.12198688, 0.0447212 ]),\n",
              " 'std_test_score': array([0.00582464, 0.00502712, 0.00595468, 0.00242207, 0.00423145,\n",
              "        0.00321688, 0.00400347, 0.00303539])}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_LW5HL1f30_",
        "outputId": "4e5aefef-f4b8-4648-e801-916f8cf05c4a"
      },
      "source": [
        "grid_search2.best_estimator_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words='english', strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=False)),\n",
              "                ('model',\n",
              "                 MultinomialNB(alpha=0.001, class_prior=None,\n",
              "                               fit_prior=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m37PzNUf31A",
        "outputId": "84e3786d-2558-4f4b-c938-5b10f93645a6"
      },
      "source": [
        "predicted2 = grid_search2.predict(twenty_test.data)\n",
        "\n",
        "print(classification_report(twenty_test.target,\n",
        "                            predicted2, \n",
        "                            target_names=twenty_test.target_names))\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(twenty_test.target, predicted2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.85      0.81      0.83       319\n",
            "           comp.graphics       0.66      0.74      0.70       389\n",
            " comp.os.ms-windows.misc       0.72      0.63      0.67       394\n",
            "comp.sys.ibm.pc.hardware       0.65      0.72      0.68       392\n",
            "   comp.sys.mac.hardware       0.83      0.82      0.83       385\n",
            "          comp.windows.x       0.83      0.76      0.80       395\n",
            "            misc.forsale       0.80      0.82      0.81       390\n",
            "               rec.autos       0.89      0.89      0.89       396\n",
            "         rec.motorcycles       0.94      0.96      0.95       398\n",
            "      rec.sport.baseball       0.96      0.93      0.94       397\n",
            "        rec.sport.hockey       0.94      0.97      0.96       399\n",
            "               sci.crypt       0.89      0.94      0.91       396\n",
            "         sci.electronics       0.80      0.74      0.77       393\n",
            "                 sci.med       0.90      0.83      0.86       396\n",
            "               sci.space       0.87      0.91      0.89       394\n",
            "  soc.religion.christian       0.87      0.93      0.90       398\n",
            "      talk.politics.guns       0.78      0.89      0.83       364\n",
            "   talk.politics.mideast       0.97      0.93      0.95       376\n",
            "      talk.politics.misc       0.76      0.66      0.70       310\n",
            "      talk.religion.misc       0.70      0.67      0.68       251\n",
            "\n",
            "                accuracy                           0.83      7532\n",
            "               macro avg       0.83      0.83      0.83      7532\n",
            "            weighted avg       0.83      0.83      0.83      7532\n",
            "\n",
            "Accuracy: 0.8325809877854488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dhNneElf31A"
      },
      "source": [
        "Increased from 0.816"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfmPZd-wf31A",
        "outputId": "fe8d8eae-389d-485f-e352-89f9cf09f04f"
      },
      "source": [
        "0.832 - 0.816"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.016000000000000014"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T11nwTof31B"
      },
      "source": [
        "1 or 2 more documents classified correctly as a result of hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a62i4dvtf31B"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Productionalize the classification model:\n",
        "- Save the model to file\n",
        "- Implement a web service that will load the model\n",
        " * Make prediction when a request is received using model\n",
        " * return the prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QkqDAvqf31B",
        "outputId": "fe2e1c24-24c1-4420-ab00-c92fe1ed49b1"
      },
      "source": [
        "5+6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XNTc_tRf31B",
        "outputId": "7becdf51-4c03-47ba-b15e-8f2d6dbb0dd3"
      },
      "source": [
        "grid_search2.predict([\"Fast Automobiles are fun to drive\",\"ethernet cables suck\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y__BpmxLf31B"
      },
      "source": [
        "## Save model to disk for later reuse\n",
        "You don't want to retrain each time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1eNEaRcf31B",
        "outputId": "987d811d-875d-4b74-9c87-56858dcff679"
      },
      "source": [
        "%pip install joblib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHl9GIymf31C",
        "outputId": "eea40520-bcb3-4db5-ed7e-39895943ebf3"
      },
      "source": [
        "import joblib\n",
        "joblib.dump(grid_search2.best_estimator_,\"email_classifier.joblib\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['email_classifier.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3cEfbVof31C"
      },
      "source": [
        "email_classifier_model = joblib.load(\"email_classifier.joblib\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbUwcty1f31C",
        "outputId": "ff27b189-4ee5-44b4-a0aa-1a6bed12aaa1"
      },
      "source": [
        "### And use it to classifiy, like above\n",
        "email_classifier_model.predict([\"Fast Automobiles are fun to drive\",\"ethernet cables suck\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}